{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "- `pandas`\n",
    "- Where to find data?\n",
    "   - Web Scraping & APIs\n",
    "   \n",
    "   \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/COGS108/Lectures-Fa25/blob/main/03-Pandas.ipynb)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/COGS108/Lectures-Wi22/main/02_python/img/pandas.png\" alt=\"pandas\" width=\"600px\">\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is Python library for managing heterogenous data.\n",
    "\n",
    "At its core, Pandas is used for the **DataFrame** object, which is:\n",
    "- a data structure for labeled rows and columns of data\n",
    "- associated methods and utilities for working with data.\n",
    "- each column contains a `pandas` **Series**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After this\n",
    "\n",
    "When you are done with this exercise/lecture... the real learning can begin!\n",
    "\n",
    "One of your best tools is https://pandastutor.com\n",
    "\n",
    "Another is the documentation for pandas including https://pandas.pydata.org/docs/user_guide/10min.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder about tab completion and contextual help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a csv file of data\n",
    "df = pd.read_csv('data/my_data.csv')\n",
    "print('total elements:',df.size,'\\nshape of table',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out a few rows or last few rows of the dataframe using head() or tail()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrame:\n",
    "- Index for each row\n",
    "- Column name for each column (Series)\n",
    "- Stores heterogenous types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing (Indexing): select a Series (column) using its name\n",
    "df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing: select a row & column BY NAME with 'loc'\n",
    "df.loc[4, 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing: getting several rows/columns at a time\n",
    "df.loc[3:8, ['age', 'last_name'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing: getting several rows/columns at a time\n",
    "df.loc[[3, 7, 9], 'first_name':'age' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by the POSITION (not name!)\n",
    "df.iloc[4:8,2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = df.set_index('last_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df.loc[['Clark','Thomas']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #1A\n",
    "\n",
    "What would be the output of `df['age'] > 10`?\n",
    "\n",
    "- A) subset of `df` including only rows of individuals older than 10\n",
    "- B) a Boolean with `True` for rows where age is greater than 10 and `False` otherwise\n",
    "- C) `id`s of rows where observations are greater than 10 \n",
    "- D) an error\n",
    "- E) I'm super lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #1B\n",
    "\n",
    "Given the answer to 1A, how can you use that to method to select all the rows with age>50?\n",
    "\n",
    "How about with age>50 and also having a score==-1?\n",
    "\n",
    "Use the cell below to work these out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other ways to select...\n",
    "\n",
    "For simpler selection tasks you can also use:\n",
    "- .query() \n",
    "- .set_index().loc[]\n",
    "but slicing as we did above is the most general, flexible method suitable for very complicated situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('first_name == \"Andrea\" ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .set_index('first_name')\n",
    "    .loc['Andrea']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: .loc uses the index, whatever it is, \n",
    "and sometimes the index is NOT numeric!\n",
    "\n",
    "Compare this usage\n",
    "```python\n",
    "df.set_index('age').loc[52]\n",
    "```\n",
    "\n",
    "to the usage in the previous cell\n",
    "```python\n",
    "df.set_index('first_name').loc['Andrea']\n",
    "```\n",
    "\n",
    "In contrast .iloc is always positional... the i stands for index.\n",
    "Therefore\n",
    "```python\n",
    "df.set_index('age').iloc[5:15]\n",
    "```\n",
    "and\n",
    "```python\n",
    "df.set_index('first_name').iloc[5:15]\n",
    "```\n",
    "produce the same output rows -- the 5th up to but not including the 15th -- but obviously with a different index in each case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking out the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how large our dataframe is\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what columns we have in our DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('first_name').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the datatypes of our variables\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "\n",
    "- quantitative (numbers)\n",
    "- qualitative (categorical)\n",
    "- basic descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking categorical data\n",
    "df['first_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a particular descriptive statistic, like mean(), median(), mode(), std(), var(), etc\n",
    "df[['id','age', 'value']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe a particular column\n",
    "df['value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics of all numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #2A\n",
    "\n",
    "What's the average (mean) age of the individuals in this dataset?\n",
    "\n",
    "- A) 14\n",
    "- B) 46\n",
    "- C) 28730\n",
    "- D) NA\n",
    "- E) I'm super lost/unsure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #2B\n",
    "\n",
    "Use the code cell below to find the average age of adults (excluding minors age<18) in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pandas`: Common Manipulations\n",
    "\n",
    "You'll want to be *very* familiar with a few common data manipulations when wrangling data, each of which is described below:\n",
    "\n",
    "Manipulation | Description\n",
    "-------|------------\n",
    "**select** | select which columns to include in dataset\n",
    "**filter** | filter dataset to only include specified rows\n",
    "**mutate** | add a new column based on values in other columns\n",
    "**groupby** | group values to apply a function within the specified groups\n",
    "**summarize** | calculate specified summary metric of a specified variable\n",
    "**arrange** | sort rows ascending or descending order of a specified column\n",
    "**merge** | join separate datasets into a single dataset based on a common column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting & Dropping Columns\n",
    "\n",
    "- include subset of columns of larger data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which columns to include\n",
    "select_df = df[['id', 'age', 'score', 'value']]\n",
    "select_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows we don't want\n",
    "new_df = df.drop(labels=[0,2,6], axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the DataFrame after dropping, what went wrong and how do we fix it?\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data (slicing)\n",
    "\n",
    "- include a subset (slice) of rows from larger data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any data from people below the age of 18\n",
    "sum(df['age'] < 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['age'] < 18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only participants who are 18 or older  AND who have a score of -1\n",
    "df_new = df[ (df['age'] >= 18)] # & (df['score']==-1)]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data (NaNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df['value'].hasn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note in class\n",
    "# can operate on entire dataframe\n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values by row\n",
    "df.isnull().sum(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the missing values\n",
    "df[df['value'].isnull()]\n",
    "# can also use .isna() or isnan()!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Data - NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with null values: Drop rows with missing data\n",
    "print('before dropping null rows',df.shape)\n",
    "no_na_df = df.dropna()\n",
    "print('new shape', no_na_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you can fill in a value for those missing values!\n",
    "df.fillna?\n",
    "# df.fillna(100) or df.fillna( df['value'].mean() ) or just df.fillna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Missing Data - Bad Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the properties of specific columns\n",
    "df['score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.score<0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the plot of the data for score to see the distribution\n",
    "df['score'].plot(kind='hist', bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Data - Bad Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for how many values have a -1 value in 'score'\n",
    "sum(df['score'] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row with -1 value in 'score'\n",
    "df = df[df['score'] != -1]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new columns (mutating)\n",
    "\n",
    "- `assign` can be very helpful in adding a new column\n",
    "- lambda functions can be used to carry out calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert age in years to age in (approximate) days\n",
    "df = df.assign(age_days = df['age'] * 365)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_months'] = df['age'] * 12\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping & summarizing\n",
    "\n",
    "- group by a particular variable\n",
    "- calculate summary statistics/metrics within group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caclculate average within each age\n",
    "df.groupby('first_name').age.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Rows (arrange)\n",
    "\n",
    "- specify order in which to display rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by values in age\n",
    "df = df.sort_values(by = ['age'],ascending=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining datasets\n",
    "![image of join operations](https://raw.githubusercontent.com/COGS108/Lectures-Fa24/main/img/join.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create two DataFrames\n",
    "left = pd.DataFrame({'key': ['A', 'B', 'C', 'D'], 'value': np.random.randn(4)})    \n",
    "right = pd.DataFrame({'key': ['B', 'D', 'E', 'F'], 'value': np.random.randn(4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner merge\n",
    "pd.merge(left, right, on='key', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right merge\n",
    "pd.merge(left, right, on='key', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left merge\n",
    "pd.merge(left, right, on='key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer join\n",
    "pd.merge(left, right, on='key', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE this cell to figure out if .merge() has a default \"how\" argument... i.e. what happens if you leave out how='...'\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Here's our check-in for today: https://forms.gle/EpNS7BYhfAxvBM4X7\n",
    "\n",
    "No, I didn't put a QR code here because I expect you to be actively engaged in this notebook, not staring at my screen passively.  Get the notebook running and click the link above.\n",
    "\n",
    "#### Question #3\n",
    "\n",
    "If table A had 5 rows and table B had 5 rows and 3 of those rows in each table were from the same observations present in the *other* table, how many rows would be present if an **inner merge** were carried out?\n",
    "\n",
    "- A) 3\n",
    "- B) 5\n",
    "- C) 10\n",
    "- D) 13\n",
    "- E) Totally unsure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #4\n",
    "\n",
    "If table A had 5 rows and table B had 5 rows and 3 of those rows in each table were from the same observations present in the *other* table, how many rows would be present if a **left merge** were carried out?\n",
    "\n",
    "- A) 3\n",
    "- B) 5\n",
    "- C) 10\n",
    "- D) 13\n",
    "- E) Totally unsure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #5\n",
    "\n",
    "How could you take the results of the outer merge and fill in the NaNs with the last good entry above the missing number in the each column?  Hint: look at the pandas docs for .fill()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE this cell to figure out this question\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data\n",
    "\n",
    "- We'll have a whole lecture (or two) on visualization\n",
    "- For now, we'll just look at one uniquely-pandas approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all numerical columns, and their interactions\n",
    "pd.plotting.scatter_matrix(df[['age', 'score', 'value']], figsize=[12, 12], marker=12);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data directly from the internet\n",
    "\n",
    "There are several ways you can load data into Pandas dataframes directly from the internet.  You don't need to have the data stored locally at all.\n",
    "1. Use pd.read_csv(URL) or similar command to grab publicly visible files with their own HTTP address.\n",
    "2. Web scrape HTML files and their contents\n",
    "3. Use Application Program Interface (API) libraries like `gspread` to read data directly from cloud services (in this case from Google Sheets)\n",
    "4. Use REST APIs to construct HTTP requests that will return well-organized data directly without scraping\n",
    "\n",
    "Below we will show examples of some of these use cases... you can read the docs and learn more about these methods on your own!\n",
    "\n",
    "## Direct HTTP access to files\n",
    "\n",
    "GitHub and Google Drive and Dropbox can all give you sharing links for a file that will let you load the file from the cloud.  For instance you can get a file directly from its GitHub http address by adding '?raw=True' to the end of the URL.  This works nicely if you don't want to bother with cloning a repo that contains a file before loading that file from your local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrollment data for UCSD courses in Sp25\n",
    "pd.read_csv('https://github.com/jasongfleischer/UCSDHistEnrollData/blob/master/data/schedules/SP25.tsv?raw=True', sep='\\t')\n",
    "# NB: this is read_csv() but I'm using it to read a tsv file!\n",
    "# the function takes a sep argument that let's you specify the column seperator... \n",
    "# in this case the file is tab seperated between columns not comma seperated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "\n",
    "`requests` is a python library that allows you to construct and send URL requests to websites.  It will also dump back to you the HTML or other data type returned by the website.  If you're web scraping or using a REST API (see next section!) you will probably use requests or some similar tool\n",
    "\n",
    "BeatifulSoup is the standard library for webscraping in Python.  It works by giving you pattern matching tools to extract data from within HTML tags.\n",
    "\n",
    "Its easier to show an example than explain... but note that you will find lots of examples and tutorials for these if you search around!\n",
    "\n",
    "Let's start with installing some libraries that (at least on my computer) needed to be installed to do this right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get a lot of requirement already satisfied prints here that's great!\n",
    "# if there's an installation happening you may (or may not) need to restart your kernel to get these to work\n",
    "%pip install lxml html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # allows you to send URL requests and recieve the data back\n",
    "from bs4 import BeautifulSoup # the most used web scraper in python land\n",
    "\n",
    "# Request Jason's Github repositories page \n",
    "page = requests.get('https://github.com/jasongfleischer?tab=repositories')  \n",
    "\n",
    "# The content we get back is a messily organized html file\n",
    "print(page.text[:1000])  #  this goes on for a dozen pages, show only the first thousand characters of the web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using a webscraper is an exercise in frustration and detailed reading of this ugly ugly HTML mess. Modern webpages have hundreds or thousands of tags... and to scrape you need to identify which tag types are holding the data you want off the page.  This is a manual, detailed, and often frustrating process.\n",
    "\n",
    "Below you can see an example of how this can work.  First start by making a BeautifulSoup instance and feeding it the text coming out from our URL request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# Identify repository elements.\n",
    "# Inspect the HTML structure of a GitHub repositories page in your browser's developer tools. \n",
    "# Look for a common element or class that encloses each repository entry. \n",
    "# Typically, each element you want (in this case a repo in the list) \n",
    "# Will live within a <div>, <span>, <li>, <h2>, or other tag.  \n",
    "# You would then use soup.find_all() to select all these elements.\n",
    "# in our case there's a weirdly styled <li> (list item) that marks out repos today\n",
    "\n",
    "repositories = soup.find_all('li', class_='col-12 d-flex flex-justify-between width-full py-4 border-bottom color-border-muted public source')\n",
    "# You may need to adjust the class based on current GitHub structure... html will change without warning from time to time!!\n",
    "\n",
    "# Extract repository details.\n",
    "# that <li> tag was just the outer casing.  Each repo has different elements we want.\n",
    "# So we iterate through each identified repository element and extract the desired information, such as:\n",
    "#   Name: Usually within an h3 or a tag.\n",
    "#   Description: Often in a p tag.\n",
    "#   Language: Typically found within a span or li tag with a specific class.\n",
    "#   Stars: Look for elements containing star icons or star counts.\n",
    "\n",
    "repo_data = []\n",
    "for repo in repositories:\n",
    "        name_tag = repo.find('a', itemprop='name codeRepository')\n",
    "        name = name_tag.text.strip() if name_tag else \"N/A\"\n",
    "\n",
    "        description_tag = repo.find('p', itemprop='description')\n",
    "        description = description_tag.text.strip() if description_tag else \"No description\"\n",
    "\n",
    "        language_tag = repo.find('span', itemprop='programmingLanguage')\n",
    "        language = language_tag.text.strip() if language_tag else \"N/A\"\n",
    "\n",
    "        # Example for stars (may vary)\n",
    "        star_tag = repo.find('a', href=lambda href: href and 'stargazers' in href)\n",
    "        stars = star_tag.text.strip() if star_tag else \"0\"\n",
    "\n",
    "        repo_data.append({\n",
    "            'name': name,\n",
    "            'description': description,\n",
    "            'language': language,\n",
    "            'stars': stars\n",
    "        })\n",
    "\n",
    "# pretty print the resulting data we collected\n",
    "pd.DataFrame(repo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Question #5\n",
    "\n",
    "What type/format of output comes out of a call to `requests.get(URL).text`\n",
    "\n",
    "- A) CSV\n",
    "- B) HTML\n",
    "- C) JSON\n",
    "- D) API\n",
    "- E) I'm super lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Program Interface (APIs)\n",
    "\n",
    "Client-Server APIs are basically a way for your software to talk to someone elses software. The client-server part means that there is a central resource (server) that is being used by many other pieces of software (clients).  \n",
    "\n",
    "An API is well documented, telling you what kinds of data you can obtain (or submit!) and how to form your requests.  That means you should always RTFM (and you should look up that term if you don't know what it means).\n",
    "\n",
    "APIs are often (but not always) via HTTP.  These web based APIs for client-server applications are known as  Representational State Transfer APIs aka REST APIs. These RESTful APIs utilize standard HTTP methods (verbs) to perform operations on resources:\n",
    "\n",
    "- GET: Retrieves a resource or a collection of resources.\n",
    "- POST: Creates a new resource.\n",
    "- PUT: Updates an existing resource (replaces the entire resource).\n",
    "- PATCH: Partially updates an existing resource.\n",
    "- DELETE: Removes a resource.\n",
    "\n",
    "\n",
    "The `requests` library we mentioned earlier implements RESTful interfaces.  For instance you can get data from an API\n",
    "```python\n",
    "r = requests.get('https://api.github.com/events')\n",
    "```\n",
    "or post data to a webpage\n",
    "```python\n",
    "r = requests.post('https://httpbin.org/post', data={'key': 'value'})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on APIs:\n",
    "- Make sure to read the API docs and understand the guidelines! \n",
    "- These guidelines typically specify the number / rate / size of requests.  If you break those guidelines you may be banned from the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Github API\n",
    "\n",
    "One example of a REST API you can access is GitHub.  The software we use to setup the project repos for all 800 or so of you in this class uses this API.\n",
    "\n",
    "https://api.github.com/\n",
    "\n",
    "its documentation is here\n",
    "\n",
    "https://docs.github.com/en/rest?apiVersion=2022-11-28\n",
    "\n",
    "For example, the following URL will give you data about whatever username you place at the end of the request. \n",
    "\n",
    "https://api.github.com/users/some_user_name\n",
    "\n",
    "In the cell below you can see an example of sucking in data from GitHub API and spitting it out in a Pandas dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "fromn time import sleep\n",
    "\n",
    "commit_list = []\n",
    "\n",
    "# GitHub username\n",
    "username = \"jasongfleischer\"\n",
    "\n",
    "headers = {}\n",
    "# optional: GitHub token for authentication allows you to access \n",
    "# private repos and also make more requests/minute than without a token\n",
    "#\n",
    "# if you want to see how to get a token for Github API you can\n",
    "# simply search or ask an AI \"how to make a token for github API access\"\n",
    "#\n",
    "# to use a token uncomment the lines below and copy/paste your token \n",
    "# token = \"your github token here\"\n",
    "# headers = {    \"Authorization\": f\"token {token}\"}\n",
    "\n",
    "# Calculate date one year ago\n",
    "one_year_ago = (datetime.utcnow() - timedelta(days=365)).isoformat() + \"Z\"\n",
    "\n",
    "# Step 1: Get user's repositories\n",
    "repos_url = f\"https://api.github.com/users/{username}/repos\"\n",
    "repos_response = requests.get(repos_url, headers=headers)\n",
    "repos = repos_response.json()\n",
    "\n",
    "# Step 2: Get commits from the last year\n",
    "for repo in repos:\n",
    "    repo_name = repo['name']\n",
    "    commits_url = f\"https://api.github.com/repos/{username}/{repo_name}/commits\"\n",
    "    params = {\n",
    "        \"since\": one_year_ago,\n",
    "        \"author\": username  # Filter commits authored by the user\n",
    "    }\n",
    "    commits_response = requests.get(commits_url, headers=headers, params=params)\n",
    "    commits = commits_response.json()\n",
    "    sleep(0.5) # pause for half a second between requests to prevent exceeding the API rate\n",
    "    \n",
    "    for commit in commits:\n",
    "        sha = commit['sha']\n",
    "        message = commit['commit']['message']\n",
    "        date = commit['commit']['author']['date']\n",
    "        commit_list.append( {'repo': repo_name, 'hash':sha, 'timestamp':date, 'message':message} )\n",
    "\n",
    "\n",
    "# Step 3: create a DataFrame of the users commits\n",
    "commit_df = pd.DataFrame(commit_list)\n",
    "\n",
    "commit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Question #6\n",
    "\n",
    "Take a look at what's inside the variable `commits_response` that was created in the cell above.  What type/format of output is it?\n",
    "\n",
    "- A) CSV\n",
    "- B) HTML\n",
    "- C) JSON\n",
    "- D) API\n",
    "- E) I'm super lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authorization for an API\n",
    "\n",
    "APIs often require some way to show who is using the API. That allows the API to have some method of making sure that people only see data they should.  It also allows the API to track the data usage better.  \n",
    "\n",
    "Even APIs that do not *require* authorization (like GitHub) work better/faster/longer when you use them with an authorization.  If you run the GitHub request cell above too much GitHub will stop serving you data because this is non-authorized.  But look at the code above... there's a commented out version of the variable `headers` where you could provide an authorization token... and if you do that your GitHub access will be restored even after you runout of non-authorized data.\n",
    "\n",
    "Many APIs use the Open Authorization (OAuth) protocol.  This is a token based system.  You ask for a token from the server, and then use that token in future communication to show who is making the request and to verify that the requester is authorized to access the data.  A token is a bit like a password, but it isn't linked to a user per se, but usually to a piece of software. \n",
    "\n",
    "While OAuth tokens are NOT passwords, you should treat them as such.  Do **NOT** hard code the token into your code.  Essentially once you share your code with another person you are also giving them your password (a big no no!).  Instead tokens can be stored in secret files that live only on your computer (not in the repo!) or inside shell environment variables.\n",
    "\n",
    "OAuth systems are used in both APIs which you might use via HTTP (like GitHub) and non-web based ones (like `gspread` for Google Sheets direct access).  And of course there are other methods to authenticate that may be used too :)\n",
    "\n",
    "Here is a [tutorial on OAuth](https://www.digitalocean.com/community/tutorials/an-introduction-to-oauth-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping vs. APIs\n",
    "\n",
    "Web scraping and APIs are different approaches:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- APIs are an interface to interact with an application, designed for programmatic use\n",
    "    - They allow systematic, controlled access\n",
    "    - They typically return structured (friendly) data \n",
    "    - If you access via API following any rate limit rules you will not be blocked  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Web scraping (typically) involves having your bot navigate a site systematically, collecting and processing the webpage data\n",
    "    - This can be hard to systematize, being dependent on the idiosyncracies of a web page, at the time you request it\n",
    "    - This typically returns relatively unstructured data\n",
    "    - Worse, the webpage tags can and will change over time. You'll have to redo your webscraping code from time to time\n",
    "    - Much more wrangling of the data\n",
    "    - If the site notices you're scraping you will be blocked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to see more about API usage to get data, here is the documentation for GitHub's API as an example https://docs.github.com/en/rest?apiVersion=2022-11-28\n",
    "\n",
    "Below is an example of using the GitHub API and storing outputs in a Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Find Data?\n",
    "\n",
    "There are of course MANY more data sources... if you want to add your favorite you can make a pull request on this file :)\n",
    "\n",
    "* [Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets/blob/master/README.rst)\n",
    "* [Data.gov](https://catalog.data.gov/dataset)\n",
    "* [Data Is Plural](https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0)\n",
    "* [UCSD Datasets](https://ucsd.libguides.com/data-statistics/home)\n",
    "* [Datasets | Deep Learning](http://deeplearning.net/datasets/)\n",
    "* [Stanford | Social Science Data Collection](https://data.stanford.edu/)\n",
    "* [Eviction Lab (email required)](https://evictionlab.org/get-the-data/)\n",
    "* [San Diego Data](https://data.sandiego.gov/)\n",
    "* [US Census](https://www.census.gov/)\n",
    "* [Open Climate Data](http://openclimatedata.net/)\n",
    "* [Data and Story Library](https://dasl.datadescription.com/datafiles/)\n",
    "* [UCSD behavioral mobile data](http://extrasensory.ucsd.edu/)\n",
    "* [Kaggle](https://www.kaggle.com/)\n",
    "* [FiveThirtyEight](https://data.fivethirtyeight.com/)\n",
    "* [data.world](https://data.world/)\n",
    "* [Free Datasets - R and Data Mining ](http://www.rdatamining.com/resources/data)\n",
    "* [Data Sources for Cool Data Science Projects](https://blog.thedataincubator.com/2014/10/data-sources-for-cool-data-science-projects-part-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Working with Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science is Ad-Hoc\n",
    "\n",
    "- It is part of the job description to put things together that were not designed to go together.\n",
    "- We do not have universal solutions, but haphazard, idiosyncratic systems, for data collection, storage and analysis.\n",
    "- Data is everywhere. But relatively little of it was collected *as data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection, Curation, and Storage are Difficult\n",
    "\n",
    "- It can be difficult to choose broadly useful standards\n",
    "- Take time to think about your data, and how you will load, store, organize and save it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is Inherently Noisy\n",
    "\n",
    "- We live in a messy, noisy, world, with messy, noisy, people, using messy, noisy instruments.\n",
    "- There is no perfect data. \n",
    "    - There is better / or worse data, given the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Objectives\n",
    "\n",
    "- Humans and computers are different.\n",
    "- We interact with '*data*' in different ways.\n",
    "- This underlies many aspects of data wrangling\n",
    "    - The 'friendliness' of data types / files\n",
    "    - The difference between web scraping and APIs\n",
    "    - A disconnect between data in the real world, and data we want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So... What to do?\n",
    "\n",
    "- Think about how your data are stored & its structure?\n",
    "- Look at your data before you anayze it\n",
    "    - are there missing values? \n",
    "    - outlier values? \n",
    "- Are your data trustworthy? \n",
    "    - source?\n",
    "    - how was it generated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Recommendations\n",
    "\n",
    "- Prioritize using well structured, common, open file types\n",
    "    - Take advantage of existing tools to deal with these files (numpy, pandas, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look into, and then follow, common conventions\n",
    "    - Minimize custom objects, workflows and data files \n",
    "- Look for APIs. Ask if they are available.\n",
    "    - Acknowledge that web scraping and/or wrangling unstructured data are complex / long tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Think about data flow from the beginning. Organize your data pipeline, consider the 'wrangling' aspects throughout\n",
    "    - Set yourself up with well organized, labelled approach to your data\n",
    "    - Think about when and how you might want/need to save out intermediate results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
